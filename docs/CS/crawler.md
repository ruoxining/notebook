# å†™ä¸ªçˆ¬è™«CrawlerğŸ•·ï¸ï¼

è§£æhtmlçš„æ­¥éª¤ï¼š
- å±‚æ¬¡åŒ–çš„æ•°æ®
- æœ‰å¤šä¸ªè§£æhtmlçš„ä¸‰æ–¹åº“ï¼Œå¦‚ï¼šlxmlï¼Œbeautifulsoupï¼Œhtmlparser

## requests
```python
import requests
```

ååçˆ¬çš„
```python
headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36'}
```

## Beautifulsoup
### å®‰è£…å’Œåˆå§‹åŒ–
```powershell
pip install beautifulsoup4
```

è§£æçš„ç¬¬ä¸€æ­¥æ˜¯æ„å»ºä¸€ä¸ªbeautifulsoupå¯¹è±¡
```python
from bs4 import BeautifulSoup
soup = BeautifulSoup(html_doc, 'html_parser')
```

### ä»‹ç»
ç¬¬ä¸€ä¸ªå‚æ•°html_docæ˜¯è¯»å–çš„htmlæ–‡æ¡£ï¼Œç¬¬äºŒä¸ªå‚æ•°æ˜¯è§£æå™¨ï¼Œbeautifulsoupæ”¯æŒä»¥ä¸‹è§£æå™¨

| è§£æå™¨ | ä½¿ç”¨æ–¹æ³• | ä¼˜åŠ¿ | åŠ£åŠ¿ |
| --- | --- | --- | --- |
| Pythonæ ‡å‡†åº“ | BeautifulSoup(markup, â€œhtml.parserâ€) |  | ä¸­æ–‡ä¸è¡Œ |
| lxml HTMLè§£æå™¨ | BeautifulSoup(markup, â€œlxmlâ€) | å¿« | Cè¯­è¨€åº“ |
| lxml XMLè§£æå™¨ | BeautifulSoup(markup, [â€lxml-xmlâ€]), BeautifulSoup(markup, â€œxmlâ€) | å¿« | Cè¯­è¨€åº“ |
| htmlSlib | BeautifulSoup(markup, â€œhtml5libâ€) | å‡† | æ…¢ |

BeautifulSoupç±»çš„åŸºæœ¬å…ƒç´ 

| åŸºæœ¬å…ƒç´  | è¯´æ˜ |
| --- | --- |
| Tag | æ ‡ç­¾ï¼Œæœ€åŸºæœ¬çš„ä¿¡æ¯ç»„ç»‡å•å…ƒï¼Œåˆ†åˆ«ç”¨<>å’Œ</>æ ‡æ˜å¼€å¤´å’Œç»“å°¾ |
| Name | æ ‡ç­¾çš„åå­—ï¼Œå³å°–æ‹¬å·é‡Œçš„å†…å®¹ |
| Attribute | æ ‡ç­¾çš„å±æ€§ï¼Œè¿”å›ä¸€ä¸ªå­—å…¸ |
| NavigableString | æ ‡ç­¾å†…çš„éå±æ€§å­—ç¬¦ä¸²ï¼Œè¿”å›ä¸€ä¸ªString |
| Comment | æ ‡ç­¾å†…å­—ç¬¦ä¸²çš„æ³¨é‡Šéƒ¨åˆ†ï¼Œä¸€ç§ç‰¹æ®Šçš„Commentç±»å‹ |

### ä½¿ç”¨æ–¹æ³•
è®¿é—®æ ‡ç­¾ï¼šç”¨soup.<tag>çš„æ ¼å¼ï¼Œæ¯æ¬¡åªèƒ½è¿”å›ç¬¬ä¸€ä¸ªåŒ¹é…çš„tagã€‚

```python
soup = BeautifulSoup(html_doc, 'lxml')
print(soup.head) #<head><title>The Dormouse's story</title><head>
print(soup.head.title) # <title>The Dormouse's story</title>
print(soup.a) # <a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>
```


è®¿é—®å¤šä¸ªæ ‡ç­¾ï¼›ä½¿ç”¨soup.find_all(â€™<tag>â€™)ã€‚ä¼šè¿”å›ä¸€ä¸ªlist.

```python
soup.find_all('a')
// [<a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>, <a class="sister" href="http://example.com/lacie" id="link2">Lacie</a>, <a class="sister" href="http://example.com/tillie" id="link3">Tillie</a>]
soup.find_all('a')[0]
// <a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>
```

å¯ä»¥åœ¨find_allæ–¹æ³•ä¸­æ·»åŠ è¿‡æ»¤æ¡ä»¶ã€‚

```python
soup.find_all('a', text = 'Elsie')
// [<a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>]

soup.find_all('a', attrs = {'id': 'link1'})
// [<a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>]

soup.find_all('a', id = 'link1')
// ä½†æ˜¯è¿™æ ·å†™ä¸é€‚ç”¨äºæ‰€æœ‰å±æ€§ï¼Ÿ

soup.find_all('p', class_= 'title')
// [<p class="title"><b>The Dormouse's story</b></p>]
// è¿™ç§æ˜¯CSSé€‰æ‹©å™¨
```

### è®¿é—®æ ‡ç­¾å†…å®¹å’Œå±æ€§

é€šè¿‡ name å’Œ string å¯ä»¥è®¿é—®æ ‡ç­¾çš„åå­—å’Œå†…å®¹ï¼Œé€šè¿‡ get å’Œä¸­æ‹¬å·æ“ä½œç¬¦å¯ä»¥è®¿é—®æ ‡ç­¾ä¸­çš„å±æ€§å’Œå€¼ã€‚

```python
print(soup.a)
## <a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>

print(soup.a['class'])
## ['sister']

print(soup.a.get('class'))
## ['sister']

print(soup.a.name)
## 'a'

print(soup.a.string)
## 'Elsie'
```

### è§£æç½‘é¡µ

```python
import requests
import chardet
from bs4 import BeautifulSoup

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36'}
url = ""
rqg = requests.get(url, headers = hearders, timeout = 3.0)
rqg.encoding = chardet.detect(rqg.content)['encoding']  # requests è¯·æ±‚è¿‡ç¨‹

# åˆå§‹åŒ–HTML
html = rqg.content.decode('utf-8')
soup = BeautifulSoup(html, 'lxml')  # ç”Ÿæˆ BeautifulSoup å¯¹è±¡
print('è¾“å‡ºæ ¼å¼åŒ–çš„BeautifulSoupå¯¹è±¡ï¼š', soup.prettify())

print('åä¸ºtitleçš„å…¨éƒ¨å­—èŠ‚ç‚¹ï¼š', soup.find_all("title"))

print('titleå­èŠ‚ç‚¹çš„æ–‡æœ¬å†…å®¹:', soup.title.string)
print('ä½¿ç”¨get_text()è·å–çš„æ–‡æœ¬å†…å®¹ï¼š', soup.title.get())

target = soup.find_all('ul', class_ = 'menu') # æŒ‰ç…§CSSç±»åå®Œå…¨åŒ¹é…
target = soup.find_all(id = 'menu')  # ä¼ å…¥å…³é”®å­—idï¼Œæœç´¢ç¬¦åˆæ¡ä»¶çš„èŠ‚ç‚¹
target = soup.ul.find_all('a')   # æ‰€æœ‰åç§°ä¸ºaçš„èŠ‚ç‚¹
```

## lxmlçš„XPath

lxmlè¿™ä¸ªåº“åŒæ—¶æ”¯æŒHTMLå’ŒXMLçš„è§£æï¼Œæ”¯æŒXPathè§£ææ–¹å¼ï¼Œè§£ææ•ˆç‡é«˜ã€‚

ä½¿ç”¨xpathéœ€è¦ä»lxmlåº“ä¸­å€’å…¥etreeæ¨¡å—ï¼Œéœ€è¦ä½¿ç”¨htmlç±»å¯¹éœ€è¦åŒ¹é…çš„htmlå¯¹è±¡è¿›è¡Œåˆå§‹åŒ–ã€‚

```python
import requests
import chardet
from lxml import etree

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36'}
url = ""
rqg = requests.get(url, headers = hearders, timeout = 3.0)
rqg.encoding = chardet.detect(rqg.content)['encoding']  # requests è¯·æ±‚è¿‡ç¨‹

# åˆå§‹åŒ–HTML
html = rqg.content.decode('utf-8')
html = etree.HTML(html, parser = etree.HTMLParser(encoding = 'utf-8'))
```

å®‰è£…é€”å¾„

```python
pip install lxml
```

Xpathå¸¸ç”¨çš„è¡¨è¾¾å¼

| è¡¨è¾¾å¼ | æè¿° |
| --- | --- |
| nodename | é€‰å–æ­¤èŠ‚ç‚¹çš„æ‰€æœ‰å­èŠ‚ç‚¹ã€‚ |
| / | ä»æ ¹èŠ‚ç‚¹é€‰å–ã€‚ |
| // | ä»åŒ¹é…é€‰æ‹©çš„å½“å‰èŠ‚ç‚¹é€‰æ‹©æ–‡æ¡£ä¸­çš„èŠ‚ç‚¹ |
| . | é€‰å–å½“å‰èŠ‚ç‚¹ã€‚ |
| â€¦ | é€‰å–å½“å‰èŠ‚ç‚¹çš„çˆ¶èŠ‚ç‚¹ã€‚ |
| @ | é€‰å–å±æ€§ã€‚ |

ä½¿ç”¨è¡¨è¾¾å¼å®šä½headå’ŒtitleèŠ‚ç‚¹

```python
result = html.xpath('head')  ## é€šè¿‡åç§°å®šä¸ºheadèŠ‚ç‚¹
result1 = html.xpath('/html/heda/title')  ## æŒ‰èŠ‚ç‚¹å±‚çº§å®šä½titleèŠ‚ç‚¹
result2 = html.xpath('//title')  ## ä¹Ÿå¯ä»¥å®šä½titleèŠ‚ç‚¹
```

Xpathè°“è¯å¸¸ç”¨çš„è¡¨è¾¾å¼

| è¡¨è¾¾å¼ | ç»“æœ |
| --- | --- |
| xpath(â€™/body/div[1]â€™) | é€‰å–bodyä¸‹çš„ç¬¬ä¸€ä¸ªdivèŠ‚ç‚¹ |
| xpath(â€™/body/div[last()]â€™) | é€‰å–bodyä¸‹æœ€åä¸€ä¸ªdivèŠ‚ç‚¹ |
| xpath(â€™/body/div[last()-1]â€™) | é€‰å–bodyä¸‹å€’æ•°ç¬¬äºŒä¸ªdivèŠ‚ç‚¹ |
| xpath(â€™/body/div[position()<3]â€™) | é€‰å–bodyä¸‹å‰ä¸¤ä¸ªèŠ‚ç‚¹ |
| xpath(â€™/body/div[@class]â€™) | é€‰å–bodyä¸‹å¸¦æœ‰classå±æ€§çš„divèŠ‚ç‚¹ |
| xpath(/body/div[@class=â€mainâ€]â€™) | é€‰å–bodyä¸‹classå±æ€§ä¸ºmainçš„divèŠ‚ç‚¹ |
| xpath(â€/body/div[price>35]â€) | é€‰å–bodyä¸‹priceå…ƒç´ å€¼å¤§äº35çš„èŠ‚ç‚¹ |

## request-html

requests-htmlç†è§£ä¸ºå¯ä»¥è§£æHTMLæ–‡æ¡£çš„requeståº“

```python
pip install requests-html
```

è·å–ä¸€ä¸ªuser-agent

```python
user_agent = requests_html.user_agent()
```

å¯¹JavaScriptçš„æ”¯æŒæ˜¯requests-htmlæœ€å¤§çš„äº®ç‚¹ï¼Œä¼šç”¨åˆ°renderå‡½æ•°ï¼Œéœ€è¦æ³¨æ„çš„æ˜¯ç¬¬ä¸€æ¬¡ä½¿ç”¨è¿™ä¸ªæ–¹æ³•ï¼Œå®ƒä¼šå…ˆä¸‹è½½Chromiumï¼Œç„¶åä½¿ç”¨Chromiumæ¥æ‰§è¡Œä»£ç ï¼Œä½†æ˜¯ä¸‹è½½çš„æ—¶å€™å¯èƒ½éœ€è¦ä¸€ä¸ªæ¢¯å­ã€‚

ç¤ºä¾‹

```python
from requests_html import HTMLSession

session = HTMLSession()

def parse():
		r = session.get('http://www.qdaily.com/')
    # è·å–é¦–é¡µæ–°é—»æ ‡ç­¾ã€å›¾ç‰‡ã€æ ‡é¢˜ã€å‘å¸ƒæ—¶é—´
    for x in r.html.find('.packery-item'):
        yield {
            'tag': x.find('.category')[0].text,
            'image': x.find('.lazyload')[0].attrs['data-src'],
            'title': x.find('.smart-dotdotdot')[0].text if x.find('.smart-dotdotdot') else x.find('.smart-lines')[0].text,
            'addtime': x.find('.smart-date')[0].attrs['data-origindate'][:-6]
        }
```

é€šè¿‡ç®€çŸ­çš„å‡ è¡Œä»£ç ï¼Œå°±å¯ä»¥æŠŠæ•´ä¸ªé¦–é¡µçš„æ–‡ç« æŠ“å–ä¸‹æ¥ã€‚

ç¤ºä¾‹ä¸­ä½¿ç”¨çš„å‡ ä¸ªæ–¹æ³•ï¼š

â‘  find( ) å¯ä»¥æ¥æ”¶ä¸¤ä¸ªå‚æ•°ï¼š

ç¬¬ä¸€ä¸ªå‚æ•°å¯ä»¥æ˜¯classåç§°æˆ–IDç¬¬äºŒä¸ªå‚æ•°first=Trueæ—¶ï¼Œåªé€‰å–ç¬¬ä¸€æ¡æ•°æ®

â‘¡ text è·å–å…ƒç´ çš„æ–‡æœ¬å†…å®¹

â‘¢ attrs è·å–å…ƒç´ çš„å±æ€§ï¼Œè¿”å›å€¼æ˜¯ä¸ªå­—å…¸

â‘£ html è·å–å…ƒç´ çš„htmlå†…å®¹

ä½¿ç”¨requests-htmlæ¥è§£æå†…å®¹çš„å¥½å¤„åœ¨äºä½œè€…éƒ½é«˜åº¦å°è£…è¿‡äº†ï¼Œè¿è¯·æ±‚è¿”å›å†…å®¹çš„ç¼–ç æ ¼å¼è½¬æ¢ä¹Ÿè‡ªåŠ¨åšäº†ï¼Œå®Œå…¨å¯ä»¥è®©ä»£ç é€»è¾‘æ›´ç®€å•ç›´æ¥ï¼Œæ›´ä¸“æ³¨äºè§£æå·¥ä½œæœ¬èº«ã€‚